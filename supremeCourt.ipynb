{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c6cafb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gorka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gorka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gorka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\gorka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# load all metadata\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "23cfbe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los datos\n",
    "data = pd.read_csv('justice.csv')\n",
    "x_train = data['facts']\n",
    "y_train1 = data['first_party_winner']\n",
    "y_train2 = data['issue_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4f294162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimina signos de puntuacion y todo a minusculas\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b72c7c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos las stop words\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0620fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4b0de0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lematizamos\n",
    "def lemmatization(texts):\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        lemText = []\n",
    "        for sent in text:\n",
    "            stemmed = ps.stem(sent)\n",
    "            lemText.append(lem.lemmatize(stemmed))\n",
    "        texts_out.append(lemText)\n",
    "    return texts_out\n",
    "\n",
    "data_lemmatized = lemmatization(data_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c3e430a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Topic modeling\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "#lda = LdaModel(corpus=corpus, id2word=id2word, num_topics=20, random_state=100, update_every=1, chunksize=100, passes=20, alpha='auto',per_word_topics=True)\n",
    "#print(f\"TOPICOS --> {lda.print_topics()}\")\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=20, random_state=100, passes=50, eval_every=None)\n",
    "count = 0\n",
    "topics=[]\n",
    "for i in lda_model.print_topics():\n",
    "    print(\"Topic:\", count, i)\n",
    "    topics.append(i)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57483a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hay que cambiar el n_topics por el numero de topics del LDA\n",
    "n_topics = 30\n",
    "vectorized = []\n",
    "count = 0\n",
    "#Recorrer los documentos\n",
    "for i in lda_model[corpus]:\n",
    "    #Inicializar index como lista de ceros, de longitud 'n_topics'\n",
    "    index = [0]*n_topics\n",
    "    #Recorrer cada palabra de cada documento\n",
    "    for n in i:\n",
    "        #En index[8] guardamos la informacion del tópico 8\n",
    "        index[n[0]] = n[1]\n",
    "    vectorized.append(index)\n",
    "vectorized = pd.DataFrame(vectorized)\n",
    "#Filas documentos (facts)\n",
    "#Columnas topics\n",
    "print(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4075cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word cloud de documento 3298\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nDocumento = 3298\n",
    "string = ','.join(data_lemmatized[nDocumento])\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=len(data_lemmatized[0]), contour_width=3, contour_color='steelblue')\n",
    "wordcloud.generate(string)\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309bf010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud del topico 15\n",
    "l = topics[15][1]\n",
    "\n",
    "l = l.split(\"+\")\n",
    "\n",
    "lista =[]\n",
    "for x in l:\n",
    "    numero, palabra = x.split('*')\n",
    "    palabra = palabra.split('\"')[1]\n",
    "    i=0\n",
    "    while i < float(numero)*100:\n",
    "        lista.append(palabra)\n",
    "        i+=1\n",
    "lista = ','.join(lista)\n",
    "\n",
    "wordcloud.generate(lista)\n",
    "wordcloud.to_image()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766902fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud del topico 16\n",
    "l = topics[16][1]\n",
    "\n",
    "l = l.split(\"+\")\n",
    "\n",
    "lista =[]\n",
    "for x in l:\n",
    "    numero, palabra = x.split('*')\n",
    "    palabra = palabra.split('\"')[1]\n",
    "    i=0\n",
    "    while i < float(numero)*100:\n",
    "        lista.append(palabra)\n",
    "        i+=1\n",
    "lista = ','.join(lista)\n",
    "\n",
    "wordcloud.generate(lista)\n",
    "wordcloud.to_image()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8abd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparar dimensiones antes y después de hacer el pca\n",
    "from sklearn.decomposition import  PCA\n",
    "\n",
    "print(\"Before:\", vectorized.shape)\n",
    "#pca = PCA(n_components='mle', svd_solver='full')\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(vectorized)\n",
    "labels_PCA= pca.transform(vectorized)\n",
    "print(\"After:\", labels_PCA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distancia euclídea\n",
    "import math\n",
    "import numpy as np\n",
    "def getDist(clust1, clust2):\n",
    "    dist = 0\n",
    "    #Sumatorio de distancias euclideas\n",
    "    for i in range(len(clust1)):\n",
    "        #Calcular la distancia euclídea de cada i\n",
    "        dist += math.sqrt((clust1[i] - clust2[i])**2)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632bb785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generar la matriz con las distancias entre cada cluster\n",
    "def getDistances(clusters):\n",
    "    distances = []\n",
    "    for clust1 in range(len(clusters)):\n",
    "        #Convertir a numpy\n",
    "        dist_clust1 = [np.NAN] * (clust1+1)\n",
    "        for clust2 in [t for t in range(len(clusters)) if t > clust1]:\n",
    "            dist_clust1.append(getDist(clusters[clust1], clusters[clust2]))\n",
    "        distances.append(np.array(dist_clust1))\n",
    "    distances = np.array(distances)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc75896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Agrupa la fila y columna de la matriz de distancias, con la menor distancia\n",
    "data = labels_PCA.copy()\n",
    "distances = getDistances(data)\n",
    "k_clusters = [0,0,[[i] for i in range(len(data))]]\n",
    "\n",
    "tree = [[],[]]\n",
    "tree_aux = [i for i in range(len(data))]\n",
    "list_of_clusters = []\n",
    "list_of_clusters.append([k_clusters[0],k_clusters[1],k_clusters[2].copy()])\n",
    "it = 0\n",
    "n_inst = len(data)\n",
    "\n",
    "print(\"Starting\")\n",
    "\n",
    "while len(distances) > 1:\n",
    "    ind1,ind2 = np.unravel_index(np.nanargmin(distances), distances.shape)\n",
    "    min_dist = np.nanmin(distances)\n",
    "    delete = max(ind1,ind2)\n",
    "    replace = min(ind1,ind2)\n",
    "    for i in [t for t in range(len(distances)) if t != ind1 and t != ind2]:\n",
    "        distance = np.nanmax([distances[ind1][i],distances[ind2][i],distances[i][ind1],distances[i][ind2]])\n",
    "        if replace < i:\n",
    "            distances[replace][i] = distance\n",
    "        else:\n",
    "            distances[i][replace] = distance\n",
    "\n",
    "    distances = np.delete(distances, delete,1)\n",
    "    distances = np.delete(distances, delete,0)\n",
    "\n",
    "    tree[0].append([tree_aux[replace],tree_aux[delete]])\n",
    "    tree[1].append(min_dist)\n",
    "    tree_aux[replace] = it + n_inst\n",
    "    tree_aux.remove(tree_aux[delete])\n",
    "\n",
    "    k_clusters[0] = it\n",
    "    k_clusters[1] = min_dist\n",
    "    k_clusters[2][replace] = k_clusters[2][replace]+k_clusters[2][delete]\n",
    "    k_clusters[2].remove(k_clusters[2][delete])\n",
    "    it += 1\n",
    "\n",
    "    list_of_clusters.append([k_clusters[0],k_clusters[1],k_clusters[2].copy()])\n",
    "\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def plot_dendrogram(tree, labels ,mode , p, threshold):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(len(tree[0]))\n",
    "    n_samples = len(labels)\n",
    "    for i, merge in enumerate(tree[0]):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [tree[0], tree[1], counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, truncate_mode=mode, p=p , color_threshold=threshold)\n",
    "\n",
    "\n",
    "#mode : \"lastp\" p numero de nodos hoja, \"level\" p profundidad del dendograma.\n",
    "#threshold : la linea que separa los clusters.\n",
    "plot_dendrogram(tree,labels_PCA,mode =\"level\", p=3, threshold=1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_nClusters(list_clusters, k):\n",
    "    n_inst = len(list_clusters[0][2])\n",
    "    for clusters in list_clusters:\n",
    "        if len(clusters[2]) == k:\n",
    "            kClusters = [0] * n_inst\n",
    "            i = 0\n",
    "            for clust in clusters[2]:\n",
    "                for c in clust:\n",
    "                    kClusters[c] = i\n",
    "                i += 1\n",
    "            return kClusters\n",
    "    return []\n",
    "\n",
    "\n",
    "k_clusters = get_nClusters(list_of_clusters,2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_distClusters(list_clusters, d):\n",
    "    n_inst = len(list_clusters[0][2])\n",
    "    clusters_aux = list_clusters[0]\n",
    "    for clusters in list_clusters:\n",
    "        if clusters[1] > d:\n",
    "            distClusters = [0] * n_inst\n",
    "            i = 0\n",
    "            for clust in clusters_aux[2]:\n",
    "                for c in clust:\n",
    "                    distClusters[c] = i\n",
    "                i += 1\n",
    "            return distClusters\n",
    "        clusters_aux = clusters\n",
    "    return []\n",
    "\n",
    "\n",
    "dist_clusters = get_distClusters(list_of_clusters,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0591fce5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "samples = 20\n",
    "\n",
    "# Dibujar los puntos en el espacio, color: cluster, etiqueta-numérica: clase\n",
    "# Color del punto: cluster\n",
    "sc = plt.scatter(labels_PCA[:samples,0],labels_PCA[:samples,1], cmap=plt.cm.get_cmap('nipy_spectral', 10),c=k_clusters[:samples])\n",
    "plt.colorbar()\n",
    "# Etiqueta numérica: clase\n",
    "for i in range(samples):\n",
    "    plt.text(labels_PCA[i,0],labels_PCA[i,1], y_train2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba681482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "samples = 20\n",
    "\n",
    "# Dibujar los puntos en el espacio, color: cluster, etiqueta-numérica: clase\n",
    "# Color del punto: cluster\n",
    "sc = plt.scatter(labels_PCA[:samples,0],labels_PCA[:samples,1], cmap=plt.cm.get_cmap('nipy_spectral', 10),c=dist_clusters[:samples])\n",
    "plt.colorbar()\n",
    "# Etiqueta numérica: clase\n",
    "for i in range(samples):\n",
    "    plt.text(labels_PCA[i,0],labels_PCA[i,1], y_train1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a28a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosas por hacer\n",
    "#Poder meter un documento en el clúster, antes de meterlo asignarle los tópicos y hacerle pca\n",
    "#Faltan las metricas de evaluación de calidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metricas de evaluación de calidad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282037a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcular los centroides de los clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf78731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Añadir una nueva instancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b4890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clasificar la nueva instancia en un cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a59b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decir a qué cluser se le ha asignado "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}